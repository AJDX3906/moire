## **AI Agent’s Security**  
  


  


AI Agent 自身的安全也需要关注，可以预见的是，AI Agent 会深入各行各业，以后行业的基建就可能是 LLM。因此安全攻防就需要从传统软件攻防演进到`上下文攻防`，这里上下文是指 LLM 的 Context：因为现阶段AI Agent 应用主要就是在和 LLM 的上下文做交互，无论外部有多少工具调用、加了多少markdown 作为记忆，最核心的还是在筛选哪些内容进入上下文去影响 LLM 的输出。

  


传统软件的攻防中有一句话特别有意思`永远不要相信用户的输入`。这是源于**冯·诺依曼架构中的**`**指令与数据的同质性**`，因为系统无法区分数据和代码，攻击者可以利用这一漏洞将**恶意指令伪装成普通数据**输入系统，并诱骗系统执行它，从而导致例如缓冲区溢出、代码注入、反序列化等漏洞。

  


但反观 LLM 的运行机制，又何尝不是另一种`**指令与数据的同质性**`，LLM没有被微调前的原始模型像是一个`续写机器`，你给他输入什么内容，LLM就会接着你的内容开始续写。微调后才是一个问答机，但本质仍是基于 Token 概率分布的自回归生成，对输入问题的续写。这意味着 LLM 才不懂什么是 System Prompt、什么是工具输入内容、什么是记忆，只是这些内容会影响 LLM 下一个 token 输出的概率而已，没有数据和代码之分。在 LLM 的上下文窗口中，一切内容在概率计算面前都是平等的，任何输入序列都有可能偏移输出概率。理论上，用户的输入可以诱导 LLM 输出任何特定内容。

  


只要 LLM 的推理架构不发生改变（看起来应该不会变了），LLM 的`上下文攻防`会传统攻防一样，永远存在。

  


#aisecurity
